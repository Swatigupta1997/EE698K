{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import PIL\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import torchvision.utils\n",
    "import torch.utils.data\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "from scipy import misc\n",
    "from matplotlib import pyplot as plt\n",
    "# sys.path.append('yourdir/pretrained-models.pytorch') # if needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_epochs = 5\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CDATA(torch.utils.data.Dataset): # Extend PyTorch's Dataset class\n",
    "    def __init__(self, root_dir, train, transform=None):\n",
    "        # root_dir  - the root directory of the dataset\n",
    "        # train     - a boolean parameter representing whether to return the training set or the test set\n",
    "        # transform - the transforms to be applied on the images before returning them\n",
    "        #\n",
    "        # In this function store the parameters in instance variables and make a mapping\n",
    "        # from images to labels and keep it as an instance variable. Make sure to check which\n",
    "        # dataset is required; train or test; and create the mapping accordingly.\n",
    "        if train:\n",
    "            cwd=root_dir+\"/train\"\n",
    "        else :\n",
    "            cwd=root_dir+\"/test\"\n",
    "        sub_dir=os.listdir(cwd)\n",
    "#         print(sub_dir)\n",
    "#         input()\n",
    "        self.imgAdd=[]\n",
    "        self.custom_transform=transform\n",
    "        for i in range(len(sub_dir)):\n",
    "            directory=cwd+\"/\" + sub_dir[i]\n",
    "            filelist=[(directory+\"/\"+image) for image in os.listdir(directory)] \n",
    "#             print(filelist[:5])\n",
    "#             input()\n",
    "            label =np.ones(len(filelist),np.uint8)*int(sub_dir[i])\n",
    "#             print(label)\n",
    "#             input()\n",
    "#             print(labe?l[0])\n",
    "            if i==0:\n",
    "                self.imgAdd= self.imgAdd+filelist\n",
    "                self.LABEL=label\n",
    "\n",
    "            else:\n",
    "                self.imgAdd= self.imgAdd+filelist\n",
    "                self.LABEL=np.hstack((self.LABEL,label))\n",
    "            print(len(self.imgAdd))\n",
    "            print(len(self.LABEL))\n",
    "#             input()\n",
    "    def __len__(self):\n",
    "        # return the size of the dataset (total number of images) as an integer\n",
    "        # this should be rather easy if you created a mapping in __init__\n",
    "        return self.LABEL.size\n",
    "    def __getitem__(self, idx):\n",
    "        # idx - the index of the sample requested\n",
    "        #\n",
    "        # Open the image correspoding to idx, apply transforms on it and return a tuple (image, label)\n",
    "        # where label is an integer from 0-9 (since notMNIST has 10 classes)\n",
    "        \n",
    "        img=PIL.Image.open(self.imgAdd[idx])\n",
    "        \n",
    "#         print((self.LABEL[0]))\n",
    "        lbl=(self.LABEL[idx])\n",
    "        if self.custom_transform is not None:\n",
    "            img = self.custom_transform(img)\n",
    "#             img=torch.cat((img,img,img),0)\n",
    "#             print(img.shape)\n",
    "#             input()\n",
    "#         print(len(self.imgAdd))\n",
    "        \n",
    "        return img,lbl\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6482\n",
      "6482\n",
      "12964\n",
      "12964\n",
      "718\n",
      "718\n",
      "1436\n",
      "1436\n",
      "Size of train dataset: 12964\n",
      "Size of test dataset: 1436\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "cannot identify image file '/home/vijayraj/Workspace/Acads/EE698K/Project/dataset/train/0/patchN_5998.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-c0835106ca0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtrain_dataiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train images\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vijayraj/anaconda3/envs/CV2/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vijayraj/anaconda3/envs/CV2/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-62aeb3b25a71>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# where label is an integer from 0-9 (since notMNIST has 10 classes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgAdd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m#         print((self.LABEL[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vijayraj/anaconda3/envs/CV2/lib/python3.5/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2583\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2584\u001b[0m     raise IOError(\"cannot identify image file %r\"\n\u001b[0;32m-> 2585\u001b[0;31m                   % (filename if filename else fp))\n\u001b[0m\u001b[1;32m   2586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2587\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: cannot identify image file '/home/vijayraj/Workspace/Acads/EE698K/Project/dataset/train/0/patchN_5998.png'"
     ]
    }
   ],
   "source": [
    "composed_transform = transforms.Compose([transforms.Scale((224,224)),transforms.ToTensor()])\n",
    "train_dataset = CDATA(root_dir='/home/vijayraj/Workspace/Acads/EE698K/Project/dataset', train=True, transform=composed_transform) # Supply proper root_dir\n",
    "test_dataset = CDATA(root_dir='/home/vijayraj/Workspace/Acads/EE698K/Project/dataset', train=False, transform=composed_transform) # Supply proper root_dir\n",
    "\n",
    "# Let's check the size of the datasets, if implemented correctly they should be 16854 and 1870 respectively\n",
    "print('Size of train dataset: %d' % len(train_dataset))\n",
    "print('Size of test dataset: %d'% len(test_dataset))\n",
    "\n",
    "# Create loaders for the dataset\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Let's look at one batch of train and test images\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    \n",
    "train_dataiter = iter(train_loader)\n",
    "train_images, train_labels = train_dataiter.next()\n",
    "print(\"Train images\")\n",
    "imshow(torchvision.utils.make_grid(train_images))\n",
    "# input()\n",
    "test_dataiter = iter(test_loader)\n",
    "test_images, test_labels = test_dataiter.next()\n",
    "print(\"Test images\")\n",
    "imshow(torchvision.utils.make_grid(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MRCNN(nn.Module): # Extend PyTorch's Module class\n",
    "    def __init__(self, num_classes = 10):\n",
    "        super(CustomResnet, self).__init__() # Must call super __init__()\n",
    "        \n",
    "        # Define the layers of the network here\n",
    "        # There should be 17 total layers as evident from the diagram\n",
    "        # The parameters and names for the layers are provided in the diagram\n",
    "        # The variable names have to be the same as the ones in the diagram\n",
    "        # Otherwise, the weights will not load\n",
    "        self.conv1 = nn.Conv2d(3,96, kernel_size=7, stride=1,padding=0, bias=True)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2,padding=0)\n",
    "        self.conv2 = nn.Conv2d(96,160, kernel_size=3, stride=1,padding=0, bias=True)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2,padding=0)\n",
    "        sel.conv3 = nn.Conv2d(160,288,kernel_size=3, stride=1, padding=0)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=1, padding=0)\n",
    "        self.fc = nn.Linear(2592, 512)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Here you have to define the forward pass\n",
    "        # Make sure you take care of the skip connections\n",
    "        tempout = self.conv1(x)\n",
    "        tempout = self.maxpool1(tempout)\n",
    "        tempout = self.conv2(x)\n",
    "        tempout = self.maxpool2(tempout)\n",
    "        tempout = self.conv3(x)\n",
    "        tempout = self.maxpool3(tempout)\n",
    "        out = tempout.view(-1, tempout.numel())\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    # conv2d(input, weight, bias, stride, padding, dilation, groups)\n",
    "    # x = torch.cat(x_1, x_2, [dimension]) returns a Tensor x which is the concatenation of Tensors x_1 and x_2 along dimension dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 21  22  23  24 132 133 134 135 136 137 261 262 263 264 265 266 267 268\n",
      " 269 270 271 314 315 316]\n"
     ]
    }
   ],
   "source": [
    "H = 400\n",
    "Ylist = np.arange(0,400-42)\n",
    "Xlist = np.arange(0,400-42)\n",
    "xcoor = np.random.randint(42,400-42,10)\n",
    "ycoor = np.random.randint(42,400-42,10)\n",
    "\n",
    "for i in range(10):\n",
    "    Xlist[xcoor[i]-21:xcoor[i]+21] = 0\n",
    "    Ylist[ycoor[i]-21:ycoor[i]+21] = 0\n",
    "Xlist = np.unique(Xlist)\n",
    "Ylist = np.unique(Ylist)\n",
    "Xlist = Xlist[Xlist>=21]\n",
    "# for i in range(10):\n",
    "#     print(np.isin(np.arange(xcoor[i]-21,xcoor[i]+21),Xlist))\n",
    "# plt.plot(Xlist)\n",
    "# plt.show()\n",
    "print(Xlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "\n",
      "26\n",
      "48\n",
      "11\n",
      "2\n",
      "86\n",
      "73\n",
      "114\n",
      "107\n",
      "13\n",
      "108\n",
      "47\n",
      "101\n",
      "38\n",
      "104\n",
      "43\n",
      "51\n",
      "102\n",
      "96\n",
      "93\n",
      "70\n",
      "21\n",
      "118\n",
      "88\n",
      "54\n",
      "111\n",
      "74\n",
      "35\n",
      "65\n",
      "76\n",
      "41\n",
      "34\n",
      "106\n",
      "63\n",
      "53\n",
      "4\n",
      "119\n",
      "37\n",
      "14\n",
      "79\n",
      "39\n",
      "82\n",
      "25\n",
      "58\n",
      "40\n",
      "12\n",
      "27\n",
      "32\n",
      "42\n",
      "78\n",
      "66\n",
      "23\n",
      "64\n",
      "8\n",
      "45\n",
      "89\n",
      "1\n",
      "68\n",
      "59\n",
      "22\n",
      "100\n",
      "33\n",
      "61\n",
      "80\n",
      "60\n",
      "10\n",
      "18\n",
      "5\n",
      "49\n",
      "44\n",
      "16\n",
      "20\n",
      "75\n",
      "113\n",
      "31\n",
      "50\n",
      "67\n",
      "90\n",
      "92\n",
      "30\n",
      "110\n",
      "87\n",
      "97\n",
      "56\n",
      "17\n",
      "94\n",
      "72\n",
      "46\n",
      "62\n",
      "109\n",
      "28\n",
      "120\n",
      "71\n",
      "29\n",
      "36\n",
      "116\n",
      "55\n",
      "52\n",
      "3\n",
      "98\n",
      "115\n",
      "9\n",
      "6\n",
      "57\n",
      "95\n",
      "15\n",
      "83\n",
      "91\n",
      "7\n",
      "19\n",
      "24\n",
      "112\n",
      "84\n",
      "81\n",
      "85\n",
      "69\n",
      "103\n",
      "117\n",
      "77\n",
      "99\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "\n",
    "salDir = \"/home/vijayraj/Workspace/Acads/EE698K/Project/dataset/dataset1/salImgs/\"\n",
    "rawDir = \"/home/vijayraj/Workspace/Acads/EE698K/Project/dataset/dataset1/rawImgs/\"\n",
    "imgfiles=[]\n",
    "imgfiles2=[]\n",
    "imgfiles += [each for each in os.listdir(salDir) if each.endswith('.jpg')]\n",
    "imgfiles2 += [each for each in os.listdir(\"/home/vijayraj/Workspace/Acads/EE698K/Project/dataset/dataset1/rawImgs\") if each.endswith('.jpg')]\n",
    "print(imgfiles[0].split(\".\")[0][1:])\n",
    "\n",
    "# input()\n",
    "count = 0\n",
    "for file in imgfiles:\n",
    "    salImg = cv2.imread(salDir+file,0)\n",
    "    origImg = cv2.imread(rawDir+file.split(\".\")[0][1:]+\".jpg\")\n",
    "#     cv2.imshow(\"orig\",origImg)\n",
    "#     cv2.waitKey(10)\n",
    "#     print(file.split(\".\")[0][1:])\n",
    "#     input()\n",
    "    for j in [(400,400),(250,250),(150,150)]:\n",
    "#         print(j)\n",
    "        imgS = cv2.resize(salImg,j)\n",
    "        imgO = cv2.resize(origImg,j)\n",
    "#         print(imgS.shape)\n",
    "    \n",
    "        ycoorP,xcoorP = np.nonzero(imgS>255*0.9)\n",
    "        xcoorP = xcoorP[np.random.randint(xcoorP.shape[0], size=10)]\n",
    "        ycoorP = ycoorP[np.random.randint(ycoorP.shape[0], size=10)]\n",
    "        ycoorN,xcoorN = np.nonzero(imgS<255*0.1)\n",
    "        xcoorN = xcoorN[np.random.randint(xcoorN.shape[0], size=10)]\n",
    "        ycoorN = ycoorN[np.random.randint(ycoorN.shape[0], size=10)]\n",
    "#     H = 400\n",
    "#     Ylist = np.arange(0,400-42)\n",
    "#     Xlist = np.arange(0,400-42)\n",
    "#     xcoor = np.random.randint(42,400-42,10)\n",
    "#     ycoor = np.random.randint(42,400-42,10)\n",
    "    # function to get xcoorP and ycoorP\n",
    "#     for i in range(10):\n",
    "#         Xlist[xcoorP[i]-21:xcoorP[i]+21] = 0\n",
    "#         Ylist[ycoorP[i]-21:ycoorP[i]+21] = 0\n",
    "#     Xlist = np.unique(Xlist)\n",
    "#     Ylist = np.unique(Ylist)\n",
    "#     Xlist = Xlist[Xlist>=21]\n",
    "#     Ylist = ylist[Ylist>=21]\n",
    "#     xcoorN = np.random.sample(Xlist,10)\n",
    "#     ycoorN = np.random.sample(Ylist,10)\n",
    "        for i in range(10):\n",
    "            patchP1 = imgO[ycoorP[i]-21:ycoorP[i]+21,xcoorP[i]-21:xcoorP[i]+21]\n",
    "            patchP2 = cv2.flip(patchP1,0)\n",
    "            patchN1 = imgO[ycoorN[i]-21:ycoorN[i]+21,xcoorN[i]-21:xcoorN[i]+21]\n",
    "            patchN2 = cv2.flip(patchN1,0)\n",
    "            if count<=6480:\n",
    "                saveDir = \"/home/vijayraj/Workspace/Acads/EE698K/Project/dataset/train/\"\n",
    "            else :\n",
    "                saveDir = \"/home/vijayraj/Workspace/Acads/EE698K/Project/dataset/test/\"\n",
    "                \n",
    "            cv2.imwrite(saveDir + \"/1/\"+\"patchP_\"+str(count)+\".png\",patchP1)\n",
    "            cv2.imwrite(saveDir + \"/0/\"+\"patchN_\"+str(count)+\".png\",patchN1)\n",
    "            cv2.imwrite(saveDir + \"/1/\"+\"patchP_\"+str(count+1)+\".png\",patchP2)\n",
    "            cv2.imwrite(saveDir + \"/0/\"+\"patchN_\"+str(count+1)+\".png\",patchN2)\n",
    "            count= count+2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Xlist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-af0521ab665b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Xlist' is not defined"
     ]
    }
   ],
   "source": [
    "# np.random.random(Xlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2592"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
